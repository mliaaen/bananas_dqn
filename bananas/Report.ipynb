{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collectting bananas using Reinforced Q-learning\n",
    "Using action values for is a classic way of solving reinforced learning problems (RL). For small problems with limited state spaces and actions tabular methods are used. Temporal methods using Q-learning is a good way to solve these problems where the “world” we are navigating is a known place. The Bellman equation shows the update rule\n",
    "\n",
    "$$\\begin{gather*}\n",
    "  Q^*(s, \\, a) = \\mathbf{E}_{s'} \\left [r + \\gamma \\underset{a'}{\\max} \\, Q^{*} (s', a')|\\,  s ,\\, a\\right ] \\;\n",
    "\\end{gather*}$$\n",
    "\n",
    "Most real problems turn out to be much more complex and needs other methods. A novel approach is the Deep Q-learning (ref) which can be used to solve much bigger problems with e.g. bitmaps as state space. To be able to compute the state action values $Q(s,a)$ in larger state spaces they propose to use estimators (of function approximators) using neural networks to estimate the Q values. $Q^*$ is the optimal values. \n",
    "\n",
    "$$Q(s,a; \\theta) ≈ Q^* (s, a)$$\n",
    "\n",
    "The update rule for the weights in deep Q-learning looks like this.\n",
    "\n",
    "$$\\begin {gather*}\n",
    "\\Delta \\theta = \\alpha \\cdot \\overbrace{( \\underbrace{R + \\gamma \\max_a Q(S', a, \\theta^-)}_{\\rm {TD~target}} - \\underbrace{Q(S, A, \\theta)}_{\\rm {old~value}})}^{\\rm {TD~error}} \\nabla_\\theta Q(S, A, \\theta) \\;\n",
    "\\end{gather*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "### Neural network\n",
    "I have implemented my solution based on the Q-learning algorithm in the course. The neural networks (two identical) have two layer networks with one input layer, one hidden layer.\n",
    "\n",
    "\n",
    "The size of the input state is 37 and the input and hidden layer have 256 units. RELU activation is used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning using DQN\n",
    "\n",
    "The networks are trained using an error function and backpropagation. There are problems with using the neural networks directly for this since they turn out to be quite unstable since they are non-linear and that they learn correlations between the state action pairs in the training phase. \n",
    "\n",
    "The DQN method introduced two changes to mend this problem. By using a replay buffer (of s,a pairs) the training of the network is done by randomly picking a subset of experienced s,a pairs and not the actual experienced pairs of the current episode. $e_t = (s_t, a_t, r_t, s_{t+1})$ \n",
    "\n",
    "The second DQN idea is to introduce two networks such that we can update the target network only periodically to reduce the correlations with the target. The separate target network with parameters $\\theta^{-}$ for generating the targets in the Q-learning update, and it´s cloned every $C$ updates the parameters of the network $Q(s, a; \\;\\theta)$ to obtain the target network $Q(s, a; \\;\\theta^{-})$.\n",
    "\n",
    "The cloning is done with a soft update i.e. making a (close) copy using the parameter $\\tau$ using the update equation $\\theta^- = \\tau * \\theta + (1 - \\tau)*\\theta^-$\n",
    "\n",
    "The parameters I used in the solution is shown below. The solution (better than 13 over 100 epsiodes) was found in 659 episodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size 1e5\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the target network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN rewards plot \n",
    "<img src=\"dqn_learning.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning using Double DQN (DDQN)\n",
    "\n",
    "DQN uses the same local network (paramters named $\\theta$)to select and evaluate an action.\n",
    "\n",
    "\\begin{gather*}\n",
    "  Y_t = r + \\gamma Q (s', \\underset{a'}{\\arg \\max} \\, Q (s', a'; \\theta);\\theta) \\;\n",
    "\\end{gather*}\n",
    "\n",
    "where $Y_t$ corresponds to the target value. In double Q-learning (DDQN) we use both networks, $\\theta$ to select the best action and $\\theta^-$ to evaluate the action like this:\n",
    "\n",
    "\\begin{gather*}\n",
    "  Y_t = r + \\gamma Q (s', \\underset{a'}{\\arg \\max} \\, Q (s', a'; \\theta);\\theta^{-}) \\;\n",
    "\\end{gather*}\n",
    "\n",
    "Both networks must agree over the best action to the Q-value returned to be the maximum one. Learning the banana environment with this algorithm found a solution faster the DQN at 447 epsisodes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDQN rewards plot\n",
    "<img src=\"ddqn_learning.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for future work\n",
    "Since the first publication of the DQN there have been several improvements to the algorithm that can be investigated and implemented. I only implemented the \"easy\" DDQN above, but there are other improvements like:\n",
    "\n",
    "### Duelling DQN\n",
    "### Prioritized Experience Replay\n",
    "In this improvement we also keep the error each sample has. The idea is that the samples with the biggest errors are the ones we can learn the most from. Introducing this concept will need two more parameters to control the stochastic sampling (non-uniform) and avoid stopping exploring the state space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
